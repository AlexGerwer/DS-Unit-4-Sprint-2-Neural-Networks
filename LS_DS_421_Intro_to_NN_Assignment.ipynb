{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:  This represents the data on which the neural network is applied.  It is the initial data that is brought into the neural network for further processing by subsequent layers of artificial neurons. The input layer is at the very beginning of the workflow for the artificial neural network.  It is sometimes said to consist of “passive” neurons that do not take in information from previous layers because they are the very first layer of the network.  There is only one input layer in a neural network.\n",
    "\n",
    "### Hidden Layer:  This is a layer in between the input layers and output layers, where artificial neurons take in a set of weighted inputs and produce an output through an activation function. It is a typical part of nearly any neural network. This intermediate layer is the place where all of the computation is done.  There can be multiple hidden layers in a neural network.\n",
    "\n",
    "### Output Layer:  This is the last layer of neurons that produce the results of applying the neural network. Though they are made much like other artificial neurons in the neural network, output layer neurons may be built or observed in a different way, given that they are the last “actor” nodes on the network.  There is only one output layer in a neural network.\n",
    "\n",
    "### Neuron:  A traditional artificial neuron is composed of some weighted inputs, a transformation function and activation function corresponding to the biological neuron’s axon. (However, output layer neurons may be designed differently in order to streamline and improve the end results of the iterative process.)  The neuron works to adjust the weights based on lots of examples of inputs and outputs.  A neuron adjusts the weights based on the inputs and the desired outputs.  In general, a neuron:\n",
    "    1) Takes the inputs and multiplies them by their weights,\n",
    "    2) Sums up the result of the above defined multiplication,\n",
    "    3) Applies the activation function to the sum.\n",
    "\n",
    "\n",
    "### Weight:  Weights capture how important is each features in making a final decision and if each feature is increasing or decreasing the probability of the thesis.  The weights that contribute more to the overall “error” will have larger derivation values, which means that they will change more (when computing Gradient descent).\n",
    "\n",
    "### Activation Function:  The activation function defines if given node should be “activated” or how “active” a given node will be based on the weighted sum mentioned in item (2) under neuron.  \n",
    "\n",
    "### Node Map:  A node map functions similar to a program's code.  It provides a rerpresentation of the flow of information through a neural network.  Perceptrons can be viewed as building blocks in a single layer in a neural network containing all three of the functions indicated in the defnition of neuron above.  \n",
    "\n",
    "\n",
    "### Perceptron:  A perceptron is the smallest fully functional unit of a neural network.  The goal of a perceptron is to determine from the input whether the feature it is recognizing is true, in other words whether the output is going to be a 0 or 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### Your Answer Here   Information generally flows only from the input to the output layers but in some types of neural networks there are more intricate connections, such as feedback paths.  The nodes of the input layer are passive, because they do not modify the data. They receive a single value on their input, and duplicate the value to their multiple outputs.  Each value from the input layer is duplicated and sent to all of the hidden nodes. This is called a fully interconnected structure.   The values entering a hidden node are multiplied by weights, a set of predetermined numbers stored in the program. The weighted inputs are then added to produce a single number.   Before leaving the node, this number is passed through a nonlinear mathematical function called a sigmoid. This is an \"s\" shaped curve that limits the node's output. That is, the input to the sigmoid is a value between -∞ and +∞, while its output can only be between 0 and 1.  The active nodes of the output layer combine and modify the data to produce the two output values of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x0  x1  x2\n",
      "0   1   0   0\n",
      "1   1   1   0\n",
      "2   1   0   1\n",
      "3   1   1   1 \n",
      "\n",
      "   y\n",
      "0  1\n",
      "1  1\n",
      "2  1\n",
      "3  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# x0 is added a dummy variable for the bias term\n",
    "data = { 'x0': [1,1,1,1],\n",
    "         'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "df_x = df.drop(['y'], axis=1)\n",
    "print(df_x.head(), '\\n')\n",
    "df_y = df.drop(['x0', 'x1', 'x2'], axis=1)\n",
    "print(df_y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training the Perceptron\n",
    "#\n",
    "# x:   feature data\n",
    "# y:   outputs\n",
    "# z:   threshold\n",
    "# eta: learning rate\n",
    "# t:   number of iterations\n",
    "\n",
    "def perceptron_train(x, y, z, eta, t):\n",
    "\n",
    "    # Initializing parameters for the Perceptron\n",
    "    w = np.zeros(len(x.iloc[0]))        # initial weights\n",
    "    n = 0                          \n",
    "\n",
    "    # Initializing additional parameters to compute sum-of-squared errors\n",
    "    yhat_vec = np.ones(len(y))     # vector for predictions\n",
    "    errors = np.ones(len(y))       # vector for errors (actual - predictions)\n",
    "    J = []                         # vector for the SSE cost function\n",
    " \n",
    "    while n < t:                                  \n",
    "        for i in range(0, len(x)):                 \n",
    "\n",
    "            # summation step\n",
    "            f = np.dot(x.iloc[i], w)                      \n",
    "\n",
    "            # activation function\n",
    "            if f > z:                               \n",
    "                yhat = 1.                               \n",
    "            else:                                   \n",
    "                yhat = 0.\n",
    "            yhat_vec[i] = yhat                              \n",
    "\n",
    "            # updating the weights\n",
    "            for j in range(0, len(w)):             \n",
    "                w[j] = w[j] + eta*(y.iloc[i]-yhat)*x.iloc[i].iloc[j]\n",
    "\n",
    "            n += 1     \n",
    "\n",
    "        # computing the sum-of-squared errors\n",
    "        for i in range(0,len(y)):     \n",
    "            errors[i] = (y.iloc[i]-yhat_vec[i])**2\n",
    "        J.append(0.5*np.sum(errors))\n",
    "\n",
    "    # function returns the weight vector, and sum-of-squared errors        \n",
    "    return w, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights are:\n",
      "[162.5 -12.5 -25. ] \n",
      "\n",
      "The sum-of-squared erros are:\n",
      "[1.5, 1.5, 2.0, 1.0, 1.5, 1.5, 1.0, 1.5, 1.5, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#  Change the learning rate to 0.5\n",
    "z = 125.0    # threshold\n",
    "eta = 12.5   # learning rate\n",
    "t = 150     # number of iterations\n",
    "\n",
    "print (\"The weights are:\")\n",
    "print (perceptron_train(df_x, df_y, z, eta, t)[0], \"\\n\")\n",
    "\n",
    "print (\"The sum-of-squared erros are:\")\n",
    "print (perceptron_train(df_x, df_y, z, eta, t)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Perceptron Convergence')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "J = perceptron_train(df_x, df_y, z, eta, t)[1]     # pulling out the sum-of-squared errors from the tuple\n",
    "epoch = np.linspace(1,len(J),len(J))\n",
    "\n",
    "%matplotlib inline  \n",
    "plt.plot(epoch, J)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Sum-of-Squared Error')\n",
    "plt.title('Perceptron Convergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the sum of squared error drops to near zero at 12 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASG\\.conda\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500745</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396423</td>\n",
       "      <td>0.116567</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.919598</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347243</td>\n",
       "      <td>0.253629</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.447236</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.418778</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688442</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.198582</td>\n",
       "      <td>0.642325</td>\n",
       "      <td>0.943638</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Pregnancies   Glucose BloodPressure SkinThickness   Insulin       BMI  \\\n",
       "0    0.352941  0.743719      0.590164      0.353535  0.000000  0.500745   \n",
       "1    0.058824  0.427136      0.540984      0.292929  0.000000  0.396423   \n",
       "2    0.470588  0.919598      0.524590      0.000000  0.000000  0.347243   \n",
       "3    0.058824  0.447236      0.540984      0.232323  0.111111  0.418778   \n",
       "4    0.000000  0.688442      0.327869      0.353535  0.198582  0.642325   \n",
       "\n",
       "  DiabetesPedigreeFunction       Age  \n",
       "0                 0.234415  0.483333  \n",
       "1                 0.116567  0.166667  \n",
       "2                 0.253629  0.183333  \n",
       "3                 0.038002  0.000000  \n",
       "4                 0.943638  0.200000  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Get list of features\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled_df = scaler.fit_transform(diabetes.drop(['Outcome'], axis=1))\n",
    "X = pd.DataFrame(scaled_df, columns=[feats])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-131-b314cd7c2d54>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-131-b314cd7c2d54>\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    weight[1:] += delta_w * adjusted\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, rate = 0.01, niter = 10):\n",
    "        self.rate = rate\n",
    "        self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        sx = sigmoid(x)\n",
    "        return sx*(1 - sx)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "    \n",
    "        # Randomly Initialize Weights\n",
    "        # Xfloat = map(lambda x: float(x),X)\n",
    "        self.weight = np.zeros(1 + X.shape[1])\n",
    "    \n",
    "        # Number of misclassifications\n",
    "        self.errors = []  # Number of misclassifications\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            err = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                # Weighted sum of inputs / weights\n",
    "                delta_w = float(self.rate) * (float(target) - float(self.predict(xi))\n",
    "                # self.weight[1:] += delta_w * xi\n",
    "                self.weight[1:] += delta_w * adjusted\n",
    "                self.weight[0] += delta_w\n",
    "                # err += int(delta_w != 0.0)\n",
    "                correct_outputs += int(delta_w != 0.0)\n",
    "                # Activation\n",
    "                activated_output = sigmoid(self.weight[0])\n",
    "                err = correct_outputs - activated_output\n",
    "                adjusted = err*sigmoid_derivative(activated_output)\n",
    "            # Calculate error\n",
    "            self.errors.append(err)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "      \"\"\"Calculate net input\"\"\"\n",
    "      return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "      \"\"\"Return class label after unit step\"\"\"\n",
    "      return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('float64') to dtype('<U32') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-474ace50ddfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# from perceptron import Perceptron\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epochs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-121-3b278f18f4a4>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;31m# Weighted sum of inputs / weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0mdelta_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                 \u001b[1;31m# self.weight[1:] += delta_w * xi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdelta_w\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0madjusted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-121-3b278f18f4a4>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m       \u001b[1;34m\"\"\"Return class label after unit step\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-121-3b278f18f4a4>\u001b[0m in \u001b[0;36mnet_input\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnet_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m       \u001b[1;34m\"\"\"Calculate net input\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot cast array data from dtype('float64') to dtype('<U32') according to the rule 'safe'"
     ]
    }
   ],
   "source": [
    "# Plot the misclassification errors versus the number of epochs\n",
    "\n",
    "pn = Perceptron(0.1, 10)\n",
    "pn.fit(X, y)\n",
    "plt.plot(range(1, len(pn.errors) + 1), pn.errors, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of misclassifications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundaries\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "   # setup marker generator and color map\n",
    "   markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "   # plot the decision surface\n",
    "   x1_min, x1_max = X[:,  0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "   np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "   # plot class samples\n",
    "   for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "        alpha=0.8, c=cmap(idx),\n",
    "        marker=markers[idx], label=cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier=pn)\n",
    "plt.xlabel('has diabetes')\n",
    "plt.ylabel('does not have diabetes')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
